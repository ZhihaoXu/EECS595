{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import os\n",
    "# train_pos_path = 'HW1_data/training/pos'\n",
    "# train_pos_files = os.listdir(train_pos_path)\n",
    "# train_pos = []\n",
    "# for file in train_pos_files:\n",
    "#     if not os.path.isdir(train_pos_path + \"/\" + file):\n",
    "#         f = open(train_pos_path + \"/\" + file, encoding='ISO-8859–1')\n",
    "#         line = f.readline()\n",
    "#         train_pos.append(line)\n",
    "#         f.close()\n",
    "\n",
    "def load_data(path):\n",
    "    sentence_files = os.listdir(path)\n",
    "    sentences = []\n",
    "    for file in sentence_files:\n",
    "        if not os.path.isdir(path + \"/\" + file):\n",
    "            f = open(path + \"/\" + file, encoding='ISO-8859–1')\n",
    "            line = f.readline()\n",
    "            sentences.append(line)\n",
    "            f.close()\n",
    "    return sentences\n",
    "train_pos = load_data('HW1_data/training/pos')\n",
    "train_neg = load_data('HW1_data/training/neg')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import nltk\n",
    "from tqdm import tqdm\n",
    "# nltk.download('punkt')\n",
    "# train_pos_dict = {}\n",
    "# tokenizer = nltk.RegexpTokenizer(r'[a-zA-Z]+')\n",
    "# for sentence in tqdm(train_pos):\n",
    "#     tokens = tokenizer.tokenize(sentence)\n",
    "#     for token in tokens:\n",
    "#         if token in train_pos_dict.keys():\n",
    "#             train_pos_dict[token] += 1\n",
    "#         else:\n",
    "#             train_pos_dict[token] = 1\n",
    "\n",
    "# V_pos = len(train_pos_dict.keys())\n",
    "# Count_pos = sum(train_pos_dict.values())\n",
    "# train_pos_prob = {}\n",
    "# for token in train_pos_dict.keys():\n",
    "#     train_pos_prob[token] = (train_pos_dict[token] + 1) / (Count_pos + V_pos)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def token_count(sentences):\n",
    "    count_dict = {}\n",
    "    tokenizer = nltk.RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            if token in count_dict.keys():\n",
    "                count_dict[token] += 1\n",
    "            else:\n",
    "                count_dict[token] = 1\n",
    "\n",
    "    V = len(count_dict.keys())\n",
    "    Count = sum(count_dict.values())\n",
    "    prob_dict = {}\n",
    "    for token in count_dict.keys():\n",
    "        prob_dict[token] = (count_dict[token] + 1) / (Count + V)\n",
    "    prob_dict['not_matched'] = 1 / (Count + V)\n",
    "    return prob_dict\n",
    "train_pos_dict = token_count(train_pos)\n",
    "train_neg_dict = token_count(train_neg)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 554/554 [00:00<00:00, 2481.64it/s]\n",
      "100%|██████████| 552/552 [00:00<00:00, 3109.63it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def train(training_path):\n",
    "    # load and preprocess data\n",
    "    train_pos = load_data(training_path + '/pos')\n",
    "    train_neg = load_data(training_path + '/neg')\n",
    "\n",
    "    # compute prior\n",
    "    prior = {}\n",
    "    prior['pos'] = len(train_pos)/(len(train_pos) + len(train_neg))\n",
    "    prior['neg'] = len(train_neg)/(len(train_pos) + len(train_neg))\n",
    "\n",
    "    # prior conditional probability\n",
    "    train_pos_dict = token_count(train_pos)\n",
    "    train_neg_dict = token_count(train_neg)\n",
    "\n",
    "    trained_model = []\n",
    "    trained_model.append(prior)\n",
    "    trained_model.append(train_pos_dict)\n",
    "    trained_model.append(train_neg_dict)\n",
    "    return trained_model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "trained_model = train(\"HW1_data/training\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 554/554 [00:00<00:00, 2903.78it/s]\n",
      "100%|██████████| 552/552 [00:00<00:00, 3331.26it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "test_data_pos = load_data(\"HW1_data/testing\" + '/pos')\n",
    "test_data_neg = load_data(\"HW1_data/testing\" + '/neg')\n",
    "test_data = test_data_pos + test_data_neg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "from math import log\n",
    "log(trained_model[0]['neg'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.6949571358051848"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from math import log\n",
    "tokenizer = nltk.RegexpTokenizer(r'[a-zA-Z]+')\n",
    "pred = []\n",
    "for sentence in tqdm(test_data):\n",
    "    pos_prob = log(trained_model[0]['pos'])\n",
    "    neg_prob = log(trained_model[0]['neg'])\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token in trained_model[1].keys():\n",
    "            pos_prob += log(trained_model[1][token])\n",
    "        else:\n",
    "            pos_prob += log(trained_model[1][\"not_matched\"])\n",
    "        \n",
    "        if token in trained_model[2].keys():\n",
    "            neg_prob += log(trained_model[2][token])\n",
    "        else:\n",
    "            neg_prob += log(trained_model[2][\"not_matched\"])\n",
    "    if pos_prob > neg_prob:\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(0)\n",
    "        "
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ed286f7f33a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[a-zA-Z]+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpos_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mneg_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "TESTING_PATH = \"HW1_data/testing\"\n",
    "def test(trained_model, testing_path):\n",
    "    test_data_pos = load_data(TESTING_PATH + '/pos')\n",
    "    test_data_neg = load_data(TESTING_PATH + '/neg')\n",
    "    test_data = test_data_pos + test_data_neg\n",
    "    ground_truth = [1] * len(test_data_neg) + [0]*len(test_data_neg)\n",
    "    tokenizer = nltk.RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    model_predictions = []\n",
    "    for sentence in tqdm(test_data):\n",
    "        pos_prob = log(trained_model[0]['pos'])\n",
    "        neg_prob = log(trained_model[0]['neg'])\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            if token in trained_model[1].keys():\n",
    "                pos_prob += log(trained_model[1][token])\n",
    "            else:\n",
    "                pos_prob += log(trained_model[1][\"not_matched\"])\n",
    "            \n",
    "            if token in trained_model[2].keys():\n",
    "                neg_prob += log(trained_model[2][token])\n",
    "            else:\n",
    "                neg_prob += log(trained_model[2][\"not_matched\"])\n",
    "        if pos_prob > neg_prob:\n",
    "            model_predictions.append(1)\n",
    "        else:\n",
    "            model_predictions.append(0)\n",
    "    return model_predictions, ground_truth"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "pred, truth = test(trained_model, TESTING_PATH)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 140/140 [00:00<00:00, 1332.29it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "cum = 0\n",
    "for i in range(len(pred)):  \n",
    "    if pred[i]==truth[i]:\n",
    "        cum += 1\n",
    "cum / 140"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8071428571428572"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}