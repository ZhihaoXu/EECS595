\documentclass[11pt]{article}
\usepackage{mathrsfs}
\usepackage{amsmath, amsthm}
\usepackage{amssymb,url}

\usepackage{color}
\usepackage{url}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{wrapfig} 
%\usepackage[numbers,sort&compress]{natbib}
%\usepackage{hypernat}
\usepackage{cite}
\usepackage{multirow}\usepackage{booktabs}

\oddsidemargin 0.0in 
\evensidemargin 1.0in 
\textwidth 6.5in 
%\headheight 1.0in 
\topmargin -0.6in 
\textheight 9.0in 
%\footheight 1.0in 


\title{EECS595: Natural Language Processing \\Homework 1 Written Assignment}
\author{Zhihao Xu}
\begin{document}
\maketitle

\begin{enumerate}


\item Suppose the deletion and the insertion costs are ``1'' respectively and the substitution cost is ``2''. Figure out whether {\em drive} is closer to {\em brief} or to {\em divers}.  What is the minimum edit distance in each case? 

\textbf{Solution:}

MED(drive, brief) = 4
% Table generated by Excel2LaTeX from sheet 'Sheet1'\begin{table}[htbp]  \centering    \begin{tabular}{|r|l|l|l|l|l|l|}    \toprule    \multicolumn{1}{|l|}{E} & \multicolumn{1}{r|}{5} & \multicolumn{1}{r|}{6} & \multicolumn{1}{r|}{5} & \multicolumn{1}{r|}{4} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{4} \\    \midrule    \multicolumn{1}{|l|}{V} & \multicolumn{1}{r|}{4} & \multicolumn{1}{r|}{5} & \multicolumn{1}{r|}{4} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{4} & \multicolumn{1}{r|}{5} \\    \midrule    \multicolumn{1}{|l|}{I} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{4} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{2} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{4} \\    \midrule    \multicolumn{1}{|l|}{R} & \multicolumn{1}{r|}{2} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{2} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{4} & \multicolumn{1}{r|}{5} \\    \midrule    \multicolumn{1}{|l|}{D} & \multicolumn{1}{r|}{1} & \multicolumn{1}{r|}{2} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{4} & \multicolumn{1}{r|}{5} & \multicolumn{1}{r|}{6} \\    \midrule    \multicolumn{1}{|l|}{\#} & \multicolumn{1}{r|}{0} & \multicolumn{1}{r|}{1} & \multicolumn{1}{r|}{2} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{4} & \multicolumn{1}{r|}{5} \\    \midrule          & \#    & B     & R     & I     & E     & F \\    \bottomrule    \end{tabular}%    \caption{Minimum Edit Distance for drive and brief}\end{table}%

MED(drive, divers)=3 \\
% Table generated by Excel2LaTeX from sheet 'Sheet1'\begin{table}[htbp]  \centering    \begin{tabular}{|r|l|l|l|l|l|l|l|}    \toprule    \multicolumn{1}{|l|}{E} & \multicolumn{1}{r|}{5} & \multicolumn{1}{r|}{4} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{2} & \multicolumn{1}{r|}{1} & \multicolumn{1}{r|}{2} & \multicolumn{1}{r|}{3} \\    \midrule    \multicolumn{1}{|l|}{V} & \multicolumn{1}{r|}{4} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{2} & \multicolumn{1}{r|}{1} & \multicolumn{1}{r|}{2} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{4} \\    \midrule    \multicolumn{1}{|l|}{I} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{2} & \multicolumn{1}{r|}{1} & \multicolumn{1}{r|}{2} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{4} & \multicolumn{1}{r|}{5} \\    \midrule    \multicolumn{1}{|l|}{R} & \multicolumn{1}{r|}{2} & \multicolumn{1}{r|}{1} & \multicolumn{1}{r|}{2} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{4} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{4} \\    \midrule    \multicolumn{1}{|l|}{D} & \multicolumn{1}{r|}{1} & \multicolumn{1}{r|}{0} & \multicolumn{1}{r|}{1} & \multicolumn{1}{r|}{2} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{4} & \multicolumn{1}{r|}{5} \\    \midrule    \multicolumn{1}{|l|}{\#} & \multicolumn{1}{r|}{0} & \multicolumn{1}{r|}{1} & \multicolumn{1}{r|}{2} & \multicolumn{1}{r|}{3} & \multicolumn{1}{r|}{4} & \multicolumn{1}{r|}{5} & \multicolumn{1}{r|}{6} \\    \midrule          & \#    & D     & I     & V     & E     & R     & S \\    \bottomrule    \end{tabular}%  \caption{Minimum Edit Distance for drive and divers}
  \end{table}%

Hence \textit{drive} is closer to \textit{divers} than \textit{brief}.

\vspace{10pt}

\item In the second lecture on N-grams, we introduced a very simple smoothing technique {\em Laplace Smoothing} (slide page 58). Please explain why once you add 1 to the count, the smoothed probabilities become \[p_{i}^*=\frac{c_{i}+1} {N+V}\] (where $V$ is the vocabulary size) and the discounted counts become \[c_{i}^{*} = (c_{i}+1) \times \frac{N}{N+V}\] 
 
\textbf{Solution:}

For the smoothed probabilities, $p^*_i = \frac{c_i^{'}}{N'}$. After adding 1 to the count, $c_i^{'} = c_i + 1$ and $N^{'} = \sum_{i=1}^V c_i^{'}=\sum_{i=1}^V c_i + 1 = N + V$. Hence $p^*_i = \frac{c_i^{'}}{N'} = \frac{c_i+1}{N+V}$. 

In order to compute the smoothed probabilities, we need to change both the numerator and denominator, by defining an discounted count $c^*$, we can smooth the probability only change the numerator. In addition to adding 1, we also need to multiply $c^*$ by a normalization factor $\frac{N}{N+V}$ to ensure $\sum_{i=1}^V c^*_i = N$. This can also be viewed as discounting non-zero counts in order to get the probability mass for the zero counts. For each non-zero count, the probability mass is lowered by $c_i - (c_i+1)\frac{N}{N+V} = \frac{c_i V - N}{N+V}$, which will be assigned to the zero counts.

\vspace{10pt}

\item This problem is about Good-Turing smoothing which we did not cover in the class. 
Suppose we have a vocabulary $V$ (i.e., a set of possible words). We'd like to estimate a unigram distribution $P(w)$ over $w \in V$. In the training set, we observe a total of $N$ words. Note that this training set may not include all members in the vocabulary set particularly if $N$ is small compared to $|V|$.  For any word seen $c$ times in the training sample, the Good-Turing estimate of its count is \[c^{*} = \frac{(c+1)N_{c+1}} {N_{c}} \]
where $N_{c}$ is the number of members of $V$ which are seen $c$ times in the corpus. For any $w$ which is observed in the training corpus, $P(w) = c^{*}(w)/N$, where $c^{*}(w)$ is the updated number of times $w$ is seen in the training set based on GT-smoothing. Suppose $V^{'}$ represents the set of words that are not seen in the training set. Under this definition, prove that: 

\[ \sum_{w \in V^{'} }P(w)=  \frac{N_{1}}{ N} \]

Hint: You can assume that $c$ in the training data set ranges from 1 to $m$, and
all $N_c,\forall c=1,\cdots m$ is non-zero, and $N_{m+1}=0$.

\textbf{Solution:}

By definition, $N = \sum_{c\ge 1} c N_c$
\begin{align*}
\sum_{w \in V^{'} }P(w) 
&= 1 - \sum_{w \in V}P(w) \\
&= 1 - \sum_{c\ge1} \frac{c^*}{N} N_c \\
&= 1 - \sum_{c\ge1} \frac{(c+1)N_{c+1}} {N_{c}N} N_c \\
&= \frac{N - \sum_{c\ge 1} (c+1)N_{c+1}}{N} \\
&= \frac{\sum_{c\ge 1} c N_c- \sum_{c\ge 1} (c+1)N_{c+1}}{N}  \\
&= \frac{1 \times N_1}{N} = \frac{N_1}{N}
\end{align*}


\end{enumerate}


\end{document}